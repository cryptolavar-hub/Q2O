# LLM Integration - Detailed Implementation Plan

**Date**: November 8, 2025  
**Version**: 1.0  
**Status**: Planning Phase  
**Execution Order**: Plan â†’ Demo â†’ Setup â†’ Implementation

---

## ğŸ¯ **Implementation Overview**

This document provides a **step-by-step implementation plan** for integrating LLM APIs (OpenAI GPT-4, Google Gemini) into the Q2O agent system.

### **Phased Approach**

```
Phase 0: Planning & Demo (Week 0)          â† We are here
         â”œâ”€â”€ B) Detailed implementation plan
         â”œâ”€â”€ D) Proof-of-concept demo
         â””â”€â”€ C) API keys and test connections

Phase 1: CoderAgent Integration (Week 1)    â† Core implementation
         â”œâ”€â”€ LLMService utility class
         â”œâ”€â”€ CoderAgent enhancement
         â”œâ”€â”€ Code validation
         â””â”€â”€ Testing

Phase 2: ResearcherAgent Synthesis (Week 2) â† Research enhancement
         â”œâ”€â”€ Post-search synthesis
         â”œâ”€â”€ Knowledge extraction
         â””â”€â”€ Caching

Phase 3: Full System Integration (Week 3)   â† Complete rollout
         â”œâ”€â”€ OrchestratorAgent
         â”œâ”€â”€ TestingAgent
         â”œâ”€â”€ QAAgent
         â””â”€â”€ Cost monitoring dashboard
```

---

## ğŸ“‹ **Phase 0: Planning & Demo (This Week)**

### **B) Detailed Implementation Plan** âœ… (This Document)

**What**: Complete technical specification  
**Duration**: 2-3 hours  
**Deliverables**:
- âœ… Architecture diagrams
- âœ… Code structure specifications
- âœ… API integration patterns
- âœ… Testing strategy
- âœ… Success criteria

---

### **D) Proof-of-Concept Demo** (Next)

**Objective**: Validate LLM integration works before full implementation

**What We'll Build**:
A standalone Python script that demonstrates:
1. Connecting to Gemini 1.5 Pro API
2. Generating FastAPI code for a simple endpoint
3. Validating the generated code
4. Comparing template vs LLM output
5. Measuring token usage and costs

**File Structure**:
```
demos/
â”œâ”€â”€ llm_integration_poc/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ demo_llm_codegen.py          # Main demo script
â”‚   â”œâ”€â”€ config.py                    # API configuration
â”‚   â”œâ”€â”€ .env.example                 # API key template
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ template_output.py       # Template-generated code
â”‚   â”‚   â””â”€â”€ llm_output.py            # LLM-generated code
â”‚   â””â”€â”€ results.md                   # Demo results & analysis
```

**Demo Script Features**:
```python
# demos/llm_integration_poc/demo_llm_codegen.py

import google.generativeai as genai
import os
from dotenv import load_dotenv

async def demo_llm_code_generation():
    """
    Demonstrate LLM code generation for Q2O.
    """
    print("="*70)
    print("Q2O LLM Integration - Proof of Concept")
    print("="*70)
    
    # Task: Generate a FastAPI endpoint for Stripe webhook
    task = {
        "title": "Create Stripe webhook endpoint",
        "tech_stack": ["FastAPI", "Stripe", "Pydantic"],
        "requirements": [
            "Handle stripe webhook events",
            "Verify webhook signature",
            "Process payment_intent.succeeded",
            "Return 200 status",
            "Log all events"
        ]
    }
    
    # 1. Show template-based approach
    print("\n[1/4] Template-based generation...")
    template_code = generate_from_template(task)
    print(f"âœ… Generated {len(template_code)} characters")
    save_output("template_output.py", template_code)
    
    # 2. Show LLM-based approach
    print("\n[2/4] LLM-based generation (Gemini 1.5 Pro)...")
    llm_code, usage = await generate_from_llm(task)
    print(f"âœ… Generated {len(llm_code)} characters")
    print(f"ğŸ“Š Tokens: {usage['input']} input, {usage['output']} output")
    print(f"ğŸ’° Cost: ${usage['cost']:.4f}")
    save_output("llm_output.py", llm_code)
    
    # 3. Compare outputs
    print("\n[3/4] Comparing outputs...")
    comparison = compare_code(template_code, llm_code)
    print(f"   Template: {comparison['template_score']}/100")
    print(f"   LLM:      {comparison['llm_score']}/100")
    
    # 4. Validate both
    print("\n[4/4] Validating generated code...")
    template_valid = validate_code(template_code)
    llm_valid = validate_code(llm_code)
    
    print(f"   Template valid: {'âœ…' if template_valid else 'âŒ'}")
    print(f"   LLM valid:      {'âœ…' if llm_valid else 'âŒ'}")
    
    # Summary
    print("\n" + "="*70)
    print("DEMO COMPLETE - LLM Integration Viable!")
    print("="*70)
    
    return {
        "template_code": template_code,
        "llm_code": llm_code,
        "usage": usage,
        "comparison": comparison
    }
```

**Expected Output**:
```
======================================================================
Q2O LLM Integration - Proof of Concept
======================================================================

[1/4] Template-based generation...
âœ… Generated 487 characters
ğŸ“ Saved: output/template_output.py

[2/4] LLM-based generation (Gemini 1.5 Pro)...
âœ… Generated 612 characters
ğŸ“Š Tokens: 856 input, 423 output
ğŸ’° Cost: $0.0032

[3/4] Comparing outputs...
   Template: 75/100 (functional but generic)
   LLM:      92/100 (production-ready with error handling)

[4/4] Validating generated code...
   Template valid: âœ…
   LLM valid:      âœ…

======================================================================
DEMO COMPLETE - LLM Integration Viable!
======================================================================

Key Findings:
âœ… LLM generates higher quality code (+17 points)
âœ… LLM handles edge cases templates miss
âœ… Cost: $0.003 per generation (negligible)
âœ… Gemini 1.5 Pro is fast (~2 seconds)
âœ… Both approaches produce valid code

Recommendation: Proceed with Phase 1 implementation
```

**Success Criteria**:
- âœ… Gemini API connection works
- âœ… LLM generates valid Python code
- âœ… Cost is as calculated ($0.003-0.005 per call)
- âœ… Quality matches or exceeds templates
- âœ… Response time < 5 seconds

**Duration**: 3-4 hours  
**Deliverable**: Working demo + analysis report

---

### **C) Set Up API Keys and Test Connections** (After Demo)

**Objective**: Configure production API access for all environments

**Tasks**:

**1. Create API Accounts**:
- [ ] Google AI Studio account (Gemini API)
- [ ] OpenAI account (GPT-4 API)
- [ ] Anthropic account (Claude - optional)

**2. Generate API Keys**:
```bash
# Gemini API Key
1. Go to: https://makersuite.google.com/app/apikey
2. Create new API key
3. Copy key (starts with "AIzaSy...")

# OpenAI API Key
1. Go to: https://platform.openai.com/api-keys
2. Create new secret key
3. Copy key (starts with "sk-...")
```

**3. Configure Environment Variables**:
```bash
# .env (add to root directory)
Q2O_USE_LLM=true
Q2O_LLM_PRIMARY=gemini  # or "openai"
Q2O_LLM_FALLBACK=openai  # fallback if primary fails

# Gemini Configuration
GOOGLE_API_KEY=AIzaSy...your_key_here
GEMINI_MODEL=gemini-1.5-pro
GEMINI_TEMPERATURE=0.7

# OpenAI Configuration (optional fallback)
OPENAI_API_KEY=sk-...your_key_here
OPENAI_MODEL=gpt-4-turbo
OPENAI_TEMPERATURE=0.7

# Cost Controls
Q2O_LLM_MAX_TOKENS=8192
Q2O_LLM_DAILY_LIMIT=100  # Max 100 LLM calls per day
Q2O_LLM_COST_ALERT=10.00  # Alert if daily cost > $10
```

**4. Test API Connections**:
Create `utils/test_llm_connections.py`:
```python
import os
from dotenv import load_dotenv
import google.generativeai as genai
import openai

async def test_gemini_connection():
    """Test Gemini API connection."""
    try:
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        model = genai.GenerativeModel('gemini-1.5-pro')
        
        response = await model.generate_content_async(
            "Say 'Gemini API working!' if you can read this."
        )
        
        print(f"âœ… Gemini API: {response.text}")
        return True
    except Exception as e:
        print(f"âŒ Gemini API failed: {e}")
        return False

async def test_openai_connection():
    """Test OpenAI API connection."""
    try:
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{
                "role": "user",
                "content": "Say 'OpenAI API working!' if you can read this."
            }],
            max_tokens=20
        )
        
        print(f"âœ… OpenAI API: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"âŒ OpenAI API failed: {e}")
        return False

# Run tests
if __name__ == "__main__":
    import asyncio
    
    print("Testing LLM API Connections...")
    print("="*60)
    
    results = asyncio.run(asyncio.gather(
        test_gemini_connection(),
        test_openai_connection()
    ))
    
    if all(results):
        print("\nâœ… ALL API CONNECTIONS WORKING!")
    else:
        print("\nâš ï¸ Some connections failed - check API keys")
```

**Success Criteria**:
- âœ… Both API connections work
- âœ… Environment variables loaded correctly
- âœ… API keys valid
- âœ… No authentication errors

**Duration**: 1-2 hours  
**Deliverable**: Working API connections + test script

---

## ğŸ—ï¸ **Phase 1: CoderAgent Integration (Week 1)**

### **A) Core Implementation**

**Duration**: 5-7 days  
**Complexity**: Medium-High

---

#### **Task 1: Create LLMService Utility Class**

**File**: `utils/llm_service.py`  
**Duration**: 1-2 days

**Specification**:

```python
"""
LLM Service - Unified interface for multiple LLM providers.
Handles OpenAI, Gemini, Anthropic with automatic fallback.
"""

from typing import Dict, Any, Optional, List, Literal
from enum import Enum
from dataclasses import dataclass
import os
import logging
from datetime import datetime
import google.generativeai as genai
import openai
from anthropic import Anthropic

class LLMProvider(str, Enum):
    """Supported LLM providers."""
    GEMINI = "gemini"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"

@dataclass
class LLMUsage:
    """Track token usage and costs."""
    provider: str
    model: str
    input_tokens: int
    output_tokens: int
    input_cost: float
    output_cost: float
    total_cost: float
    timestamp: datetime
    cache_hit: bool = False

@dataclass
class LLMResponse:
    """Unified LLM response format."""
    content: str
    usage: LLMUsage
    provider: str
    model: str
    success: bool
    error: Optional[str] = None

class LLMService:
    """
    Unified LLM service for Q2O agents.
    
    Features:
    - Multiple provider support (Gemini, OpenAI, Anthropic)
    - Automatic fallback on failures
    - Cost tracking and monitoring
    - Response caching
    - Rate limiting
    """
    
    def __init__(
        self,
        primary: LLMProvider = LLMProvider.GEMINI,
        fallback: Optional[LLMProvider] = LLMProvider.OPENAI,
        cache_dir: str = ".llm_cache"
    ):
        self.primary = primary
        self.fallback = fallback
        self.cache_dir = cache_dir
        
        # Initialize clients
        self._init_gemini()
        self._init_openai()
        self._init_anthropic()
        
        # Usage tracking
        self.usage_log: List[LLMUsage] = []
        self.total_cost = 0.0
        
        # Rate limiting
        self.daily_limit = int(os.getenv("Q2O_LLM_DAILY_LIMIT", "1000"))
        self.cost_alert = float(os.getenv("Q2O_LLM_COST_ALERT", "10.0"))
        self.daily_calls = 0
        self.daily_cost = 0.0
    
    def _init_gemini(self):
        """Initialize Gemini client."""
        api_key = os.getenv("GOOGLE_API_KEY")
        if api_key:
            genai.configure(api_key=api_key)
            self.gemini_model = genai.GenerativeModel(
                os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
            )
            logging.info("âœ… Gemini client initialized")
        else:
            self.gemini_model = None
            logging.warning("âš ï¸ GOOGLE_API_KEY not set")
    
    def _init_openai(self):
        """Initialize OpenAI client."""
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = openai.OpenAI(api_key=api_key)
            logging.info("âœ… OpenAI client initialized")
        else:
            self.openai_client = None
            logging.warning("âš ï¸ OPENAI_API_KEY not set")
    
    def _init_anthropic(self):
        """Initialize Anthropic client."""
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if api_key:
            self.anthropic_client = Anthropic(api_key=api_key)
            logging.info("âœ… Anthropic client initialized")
        else:
            self.anthropic_client = None
    
    async def complete(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        provider: Optional[LLMProvider] = None
    ) -> LLMResponse:
        """
        Generate completion using specified or primary provider.
        
        Args:
            system_prompt: System/instruction prompt
            user_prompt: User query
            temperature: Randomness (0.0-1.0)
            max_tokens: Maximum output tokens
            provider: Force specific provider (None = use primary)
        
        Returns:
            LLMResponse with content, usage, and metadata
        """
        # Check rate limits
        if not self._check_rate_limits():
            return LLMResponse(
                content="",
                usage=None,
                provider="",
                model="",
                success=False,
                error="Rate limit exceeded"
            )
        
        provider = provider or self.primary
        
        try:
            # Try primary provider
            if provider == LLMProvider.GEMINI and self.gemini_model:
                return await self._gemini_complete(
                    system_prompt, user_prompt, temperature, max_tokens
                )
            elif provider == LLMProvider.OPENAI and self.openai_client:
                return await self._openai_complete(
                    system_prompt, user_prompt, temperature, max_tokens
                )
            elif provider == LLMProvider.ANTHROPIC and self.anthropic_client:
                return await self._anthropic_complete(
                    system_prompt, user_prompt, temperature, max_tokens
                )
            else:
                raise ValueError(f"Provider {provider} not available")
        
        except Exception as e:
            logging.error(f"Primary provider {provider} failed: {e}")
            
            # Try fallback
            if self.fallback and self.fallback != provider:
                logging.info(f"Falling back to {self.fallback}")
                return await self.complete(
                    system_prompt, user_prompt, temperature, max_tokens, self.fallback
                )
            
            return LLMResponse(
                content="",
                usage=None,
                provider=str(provider),
                model="",
                success=False,
                error=str(e)
            )
    
    async def generate_code(
        self,
        task_description: str,
        tech_stack: List[str],
        research_context: Optional[Dict] = None,
        temperature: float = 0.3,  # Lower for code generation
        language: str = "python"
    ) -> LLMResponse:
        """
        Specialized method for code generation.
        
        Args:
            task_description: What to build
            tech_stack: Technologies to use
            research_context: Research findings
            temperature: Creativity (lower = more deterministic)
            language: Programming language
        
        Returns:
            LLMResponse with generated code
        """
        # Build system prompt for code generation
        system_prompt = f"""You are an expert {', '.join(tech_stack)} developer.

Generate production-quality {language} code with:
âœ… Complete type hints (mypy strict)
âœ… Comprehensive docstrings (Google style)
âœ… Error handling (try/except with specific exceptions)
âœ… Input validation (Pydantic if applicable)
âœ… Security best practices (no SQL injection, XSS prevention)
âœ… Logging (structured logging with context)
âœ… Best practices for {', '.join(tech_stack)}

{'Research Findings:\n' + json.dumps(research_context, indent=2) if research_context else ''}

Output ONLY the code - no explanations, no markdown formatting.
"""
        
        user_prompt = f"""Task: {task_description}

Technology Stack: {', '.join(tech_stack)}

Generate complete, production-ready implementation."""
        
        response = await self.complete(
            system_prompt,
            user_prompt,
            temperature=temperature,
            max_tokens=8192  # Longer for code
        )
        
        return response
    
    async def _gemini_complete(self, system, user, temp, max_tokens) -> LLMResponse:
        """Generate completion using Gemini."""
        # Gemini doesn't have separate system/user, combine them
        prompt = f"{system}\n\n{user}"
        
        response = await self.gemini_model.generate_content_async(
            prompt,
            generation_config=genai.GenerationConfig(
                temperature=temp,
                max_output_tokens=max_tokens
            )
        )
        
        # Calculate usage and cost
        usage = self._calculate_gemini_usage(response)
        self._track_usage(usage)
        
        return LLMResponse(
            content=response.text,
            usage=usage,
            provider="gemini",
            model="gemini-1.5-pro",
            success=True
        )
    
    async def _openai_complete(self, system, user, temp, max_tokens) -> LLMResponse:
        """Generate completion using OpenAI."""
        response = self.openai_client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-4-turbo"),
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ],
            temperature=temp,
            max_tokens=max_tokens
        )
        
        # Calculate usage and cost
        usage = self._calculate_openai_usage(response)
        self._track_usage(usage)
        
        return LLMResponse(
            content=response.choices[0].message.content,
            usage=usage,
            provider="openai",
            model=response.model,
            success=True
        )
    
    def _calculate_gemini_usage(self, response) -> LLMUsage:
        """Calculate Gemini token usage and cost."""
        # Gemini 1.5 Pro pricing
        input_cost_per_1k = 0.00125
        output_cost_per_1k = 0.005
        
        input_tokens = response.usage_metadata.prompt_token_count
        output_tokens = response.usage_metadata.candidates_token_count
        
        input_cost = (input_tokens / 1000) * input_cost_per_1k
        output_cost = (output_tokens / 1000) * output_cost_per_1k
        total_cost = input_cost + output_cost
        
        return LLMUsage(
            provider="gemini",
            model="gemini-1.5-pro",
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            total_cost=total_cost,
            timestamp=datetime.now()
        )
    
    def _calculate_openai_usage(self, response) -> LLMUsage:
        """Calculate OpenAI token usage and cost."""
        # GPT-4 Turbo pricing
        input_cost_per_1k = 0.01
        output_cost_per_1k = 0.03
        
        input_tokens = response.usage.prompt_tokens
        output_tokens = response.usage.completion_tokens
        
        input_cost = (input_tokens / 1000) * input_cost_per_1k
        output_cost = (output_tokens / 1000) * output_cost_per_1k
        total_cost = input_cost + output_cost
        
        return LLMUsage(
            provider="openai",
            model=response.model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            total_cost=total_cost,
            timestamp=datetime.now()
        )
    
    def _track_usage(self, usage: LLMUsage):
        """Track usage for monitoring and cost control."""
        self.usage_log.append(usage)
        self.total_cost += usage.total_cost
        self.daily_calls += 1
        self.daily_cost += usage.total_cost
        
        # Alert if over cost limit
        if self.daily_cost > self.cost_alert:
            logging.warning(f"âš ï¸ Daily cost limit exceeded: ${self.daily_cost:.2f}")
    
    def _check_rate_limits(self) -> bool:
        """Check if within rate limits."""
        if self.daily_calls >= self.daily_limit:
            logging.error(f"âŒ Daily limit reached: {self.daily_calls} calls")
            return False
        return True
    
    def get_usage_stats(self) -> Dict:
        """Get current usage statistics."""
        return {
            "total_calls": len(self.usage_log),
            "total_cost": self.total_cost,
            "daily_calls": self.daily_calls,
            "daily_cost": self.daily_cost,
            "by_provider": self._group_by_provider(),
            "by_agent": self._group_by_agent()
        }
```

**Success Criteria**:
- âœ… Connects to Gemini and OpenAI
- âœ… Automatic fallback works
- âœ… Cost tracking accurate
- âœ… Rate limiting functional
- âœ… Can generate code

**Deliverable**: `utils/llm_service.py` (500-600 lines)

---

#### **Task 2: Enhance CoderAgent with LLM**

**File**: `agents/coder_agent.py`  
**Duration**: 2-3 days

**Changes Required**:

```python
from utils.llm_service import LLMService, LLMProvider

class CoderAgent(BaseAgent, ResearchAwareMixin):
    def __init__(self, agent_id: str = "coder_main", **kwargs):
        super().__init__(agent_id, AgentType.CODER, **kwargs)
        self.template_renderer = get_renderer()
        
        # NEW: LLM service
        self.use_llm = os.getenv("Q2O_USE_LLM", "true").lower() == "true"
        if self.use_llm:
            self.llm_service = LLMService(
                primary=LLMProvider.GEMINI,
                fallback=LLMProvider.OPENAI
            )
            logging.info("âœ… CoderAgent: LLM integration enabled")
        else:
            self.llm_service = None
            logging.info("â„¹ï¸ CoderAgent: LLM disabled, template-only mode")
    
    async def _implement_code(self, code_structure, task) -> List[str]:
        """
        Implement code using HYBRID approach:
        1. Try template first (fast, reliable)
        2. If no template or complex, use LLM
        3. Validate all generated code
        """
        files_created = []
        
        for component in code_structure['components']:
            # Determine implementation strategy
            strategy = self._choose_strategy(component, task)
            
            if strategy == "template":
                code = await self._generate_from_template(component, task)
            elif strategy == "llm":
                code = await self._generate_from_llm(component, task)
            elif strategy == "hybrid":
                code = await self._generate_hybrid(component, task)
            else:
                raise ValueError(f"Unknown strategy: {strategy}")
            
            # Validate generated code
            if self._validate_code(code, component):
                file_path = self._write_file(component['path'], code)
                files_created.append(file_path)
                logging.info(f"âœ… Generated: {file_path} (strategy: {strategy})")
            else:
                raise ValueError(f"Generated code validation failed: {component['name']}")
        
        return files_created
    
    def _choose_strategy(self, component: Dict, task: Task) -> str:
        """
        Decide: template, LLM, or hybrid?
        
        Decision logic:
        - Template exists + component is standard â†’ "template"
        - No template + LLM enabled â†’ "llm"
        - Template exists + complex requirements â†’ "hybrid"
        - No template + LLM disabled â†’ ERROR
        """
        has_template = self.template_exists(component['template_name'])
        is_complex = component.get('complexity', 'medium') in ['high', 'very_high']
        is_novel = component.get('novel_technology', False)
        
        if has_template and not is_complex and not is_novel:
            return "template"
        
        if not has_template and self.use_llm:
            return "llm"
        
        if has_template and (is_complex or is_novel) and self.use_llm:
            return "hybrid"
        
        if not has_template and not self.use_llm:
            raise ValueError(f"No template for {component['name']} and LLM disabled")
        
        return "template"  # Default
    
    async def _generate_from_template(self, component: Dict, task: Task) -> str:
        """Generate code using template (FAST PATH)."""
        context = self._build_template_context(component, task)
        code = self.template_renderer.render(
            component['template_name'],
            context
        )
        return code
    
    async def _generate_from_llm(self, component: Dict, task: Task) -> str:
        """Generate code using LLM (ADAPTIVE PATH)."""
        # Get research context
        research_context = self.get_research_results(task)
        
        # Generate with LLM
        response = await self.llm_service.generate_code(
            task_description=component['description'],
            tech_stack=task.tech_stack,
            research_context=research_context,
            temperature=0.3,  # Low for deterministic code
            language="python"
        )
        
        if not response.success:
            raise ValueError(f"LLM generation failed: {response.error}")
        
        # Log usage
        logging.info(f"ğŸ’° LLM cost: ${response.usage.total_cost:.4f} "
                    f"({response.usage.input_tokens}+{response.usage.output_tokens} tokens)")
        
        return response.content
    
    async def _generate_hybrid(self, component: Dict, task: Task) -> str:
        """
        HYBRID: Start with template, enhance with LLM.
        
        Strategy:
        1. Generate base from template
        2. Ask LLM to enhance/adapt it
        3. Merge the results
        """
        # Generate base from template
        template_code = await self._generate_from_template(component, task)
        
        # Ask LLM to enhance
        system_prompt = f"""You are refining code generated from a template.
Enhance it with:
- Better error handling
- Edge case coverage
- Type safety improvements
- Security hardening
- Performance optimizations

Keep the overall structure but make it production-grade."""
        
        user_prompt = f"""Original code:
```python
{template_code}
```

Task requirements: {component['description']}
Tech stack: {', '.join(task.tech_stack)}

Enhance this code to production quality. Return ONLY the enhanced code."""
        
        response = await self.llm_service.complete(
            system_prompt, user_prompt, temperature=0.3, max_tokens=8192
        )
        
        if response.success:
            return self._extract_code(response.content)
        else:
            logging.warning("LLM enhancement failed, using template")
            return template_code
    
    def _validate_code(self, code: str, component: Dict) -> bool:
        """
        Validate generated code.
        
        Checks:
        1. Valid Python syntax
        2. Required imports present
        3. Expected functions/classes defined
        4. No security issues (basic checks)
        5. Type hints present
        """
        try:
            # Syntax check
            compile(code, '<string>', 'exec')
            
            # Check for required elements
            required_elements = component.get('required_elements', [])
            for element in required_elements:
                if element not in code:
                    logging.error(f"Missing required element: {element}")
                    return False
            
            # Basic security check
            dangerous_patterns = ['eval(', 'exec(', '__import__', 'os.system']
            for pattern in dangerous_patterns:
                if pattern in code:
                    logging.error(f"Security concern: {pattern} found in code")
                    return False
            
            return True
        
        except SyntaxError as e:
            logging.error(f"Syntax error in generated code: {e}")
            return False
```

**Success Criteria**:
- âœ… CoderAgent can use templates OR LLM
- âœ… Hybrid mode works (template + LLM enhancement)
- âœ… Code validation catches bad output
- âœ… Fallback to templates on LLM failure
- âœ… Usage tracked and logged

**Deliverable**: Enhanced `agents/coder_agent.py` (+200-300 lines)

---

#### **Task 3: Code Validation & Testing**

**File**: `tests/test_llm_integration.py`  
**Duration**: 1-2 days

**Test Cases**:

```python
import pytest
from agents.coder_agent import CoderAgent
from agents.base_agent import Task, AgentType

@pytest.mark.asyncio
async def test_llm_code_generation():
    """Test LLM generates valid FastAPI code."""
    agent = CoderAgent(agent_id="test_coder")
    
    task = Task(
        id="test_001",
        title="Create user model",
        description="Create FastAPI User model with SQLAlchemy",
        agent_type=AgentType.CODER,
        tech_stack=["FastAPI", "SQLAlchemy", "Pydantic"]
    )
    
    result = await agent.process_task(task)
    
    assert result.status == TaskStatus.COMPLETED
    assert len(result.metadata['implemented_files']) > 0
    
    # Validate generated code
    generated_code = read_file(result.metadata['implemented_files'][0])
    assert 'class User' in generated_code
    assert 'Base =' in generated_code  # SQLAlchemy
    assert 'def __init__' in generated_code or 'id: Mapped' in generated_code

@pytest.mark.asyncio
async def test_template_fallback():
    """Test fallback to template if LLM fails."""
    agent = CoderAgent(agent_id="test_coder")
    
    # Simulate LLM failure by using invalid API key
    original_key = os.getenv("GOOGLE_API_KEY")
    os.environ["GOOGLE_API_KEY"] = "invalid_key"
    
    task = Task(...)  # Standard task with template
    result = await agent.process_task(task)
    
    # Should still succeed using template
    assert result.status == TaskStatus.COMPLETED
    
    # Restore key
    os.environ["GOOGLE_API_KEY"] = original_key

@pytest.mark.asyncio
async def test_cost_tracking():
    """Test LLM usage cost tracking."""
    agent = CoderAgent(agent_id="test_coder")
    
    task = Task(...)
    await agent.process_task(task)
    
    stats = agent.llm_service.get_usage_stats()
    
    assert stats['total_calls'] > 0
    assert stats['total_cost'] > 0
    assert stats['total_cost'] < 1.0  # Should be < $1 for simple task

@pytest.mark.asyncio
async def test_code_validation_rejects_bad_code():
    """Test validation rejects invalid code."""
    agent = CoderAgent(agent_id="test_coder")
    
    # Invalid Python
    bad_code = "def foo( invalid syntax"
    assert not agent._validate_code(bad_code, {})
    
    # Security concern
    dangerous_code = "import os\nos.system('rm -rf /')"
    assert not agent._validate_code(dangerous_code, {})
```

**Success Criteria**:
- âœ… All tests pass
- âœ… LLM generation validated
- âœ… Template fallback works
- âœ… Cost tracking accurate
- âœ… Code validation catches issues

**Deliverable**: `tests/test_llm_integration.py` (10+ test cases)

---

#### **Task 4: Documentation & Configuration**

**Files to Create/Update**:

1. **`.env.example`** - Add LLM configuration
2. **`requirements.txt`** - Add LLM dependencies
3. **`docs/LLM_SETUP_GUIDE.md`** - How to configure
4. **`docs/LLM_USAGE_GUIDE.md`** - How agents use LLMs
5. **`README.md`** - Update with LLM capability

**Dependencies to Add**:
```
# requirements.txt additions
openai>=1.0.0
google-generativeai>=0.3.0
anthropic>=0.8.0  # optional
```

**Duration**: 1 day  
**Deliverable**: Complete documentation and setup guides

---

## ğŸ“Š **Phase 1 Success Metrics**

### **Quantitative**

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Code Quality** | 90%+ | QA score on LLM-generated code |
| **Success Rate** | 95%+ | % of LLM generations that pass validation |
| **Cost per Generation** | < $0.50 | Average cost per code file |
| **Response Time** | < 5 sec | Average LLM API response time |
| **Fallback Rate** | < 10% | % of times fallback to template needed |

### **Qualitative**

- âœ… CoderAgent generates code for technologies without templates
- âœ… LLM-generated code passes all QA checks
- âœ… Cost tracking shows accurate per-project costs
- âœ… Fallback to templates works seamlessly
- âœ… No security vulnerabilities in generated code

---

## ğŸ§ª **Testing Strategy**

### **Unit Tests**
- `test_llm_service.py` - LLM service functionality
- `test_llm_integration.py` - CoderAgent with LLM
- `test_code_validation.py` - Validation logic

### **Integration Tests**
- End-to-end: Objective â†’ LLM generation â†’ Validation
- Multi-provider fallback scenarios
- Cost tracking across multiple calls

### **Real-World Tests**
1. Generate Stripe integration (no template)
2. Generate Xero API client (no template)
3. Generate GraphQL schema (no template)
4. Compare quality: Template vs LLM vs Hybrid

**Success**: All 4 test projects generate working, production-quality code

---

## ğŸ“ˆ **Phase 1 Timeline**

| Week | Days | Tasks | Deliverables |
|------|------|-------|--------------|
| **Week 0** | 1-2 | Plan + Demo + Setup | This plan + POC demo + API keys |
| **Week 1** | 3-5 | LLMService + CoderAgent | Working LLM integration |
| **Week 1** | 6-7 | Testing + Documentation | Tests passing + docs complete |

**Total Duration**: **7-9 days**

---

## ğŸ”„ **Phase 2 & 3 Preview**

### **Phase 2: ResearcherAgent Synthesis** (Week 2)
- LLM synthesizes research results
- Extracts key findings and code examples
- Structures knowledge for CoderAgent

### **Phase 3: Full System Integration** (Week 3)
- OrchestratorAgent: LLM breaks down objectives
- TestingAgent: LLM generates comprehensive tests
- QAAgent: LLM analyzes code quality
- Cost monitoring dashboard

**Total Project Duration**: **3 weeks**

---

## âš™ï¸ **Configuration Management**

### **Environment Variables**

```bash
# Feature Flags
Q2O_USE_LLM=true                    # Enable/disable LLM integration
Q2O_LLM_PRIMARY=gemini              # Primary provider
Q2O_LLM_FALLBACK=openai             # Fallback provider

# API Keys
GOOGLE_API_KEY=AIzaSy...            # Gemini API key
OPENAI_API_KEY=sk-...               # OpenAI API key
ANTHROPIC_API_KEY=sk-ant-...        # Anthropic API key (optional)

# Model Selection
GEMINI_MODEL=gemini-1.5-pro         # or gemini-1.5-flash
OPENAI_MODEL=gpt-4-turbo            # or gpt-4-0125-preview
ANTHROPIC_MODEL=claude-3-opus       # or claude-3-sonnet

# Generation Parameters
Q2O_LLM_TEMPERATURE=0.7             # Creativity (0.0-1.0)
Q2O_LLM_CODE_TEMPERATURE=0.3        # Lower for code generation
Q2O_LLM_MAX_TOKENS=8192             # Maximum output tokens

# Cost Controls
Q2O_LLM_DAILY_LIMIT=1000            # Max LLM calls per day
Q2O_LLM_COST_ALERT=10.00            # Alert if daily cost > $10
Q2O_LLM_CACHE_ENABLED=true          # Cache LLM responses
Q2O_LLM_CACHE_TTL=90                # Cache TTL in days

# Debugging
Q2O_LLM_DEBUG=false                 # Log all LLM requests/responses
Q2O_LLM_DRY_RUN=false               # Simulate LLM calls (no API usage)
```

---

## ğŸ¯ **Success Criteria (Phase 1 Complete)**

### **Must Have** âœ…
- [x] LLMService class implemented and tested
- [x] CoderAgent uses LLM for code generation
- [x] Hybrid mode (template + LLM) works
- [x] Code validation prevents bad output
- [x] Cost tracking accurate
- [x] Fallback to templates reliable
- [x] All tests passing (90%+ coverage)
- [x] Documentation complete

### **Should Have** âœ…
- [x] Multi-provider support (Gemini + OpenAI)
- [x] Automatic fallback on failures
- [x] Rate limiting and cost controls
- [x] Response caching
- [x] Usage dashboard/stats
- [x] Security validation

### **Nice to Have** ğŸ“Œ
- [ ] Anthropic Claude support
- [ ] LLM output streaming
- [ ] Advanced prompt engineering
- [ ] A/B testing (template vs LLM quality)
- [ ] Cost optimization (prompt compression)

---

## ğŸ“¦ **Deliverables Checklist**

### **Code**
- [ ] `utils/llm_service.py` (500-600 lines)
- [ ] `agents/coder_agent.py` (enhanced, +200-300 lines)
- [ ] `tests/test_llm_service.py` (10+ tests)
- [ ] `tests/test_llm_integration.py` (10+ tests)
- [ ] `demos/llm_integration_poc/` (proof-of-concept)

### **Configuration**
- [ ] `.env.example` (LLM variables)
- [ ] `requirements.txt` (LLM dependencies)
- [ ] `config/llm_config.json` (model parameters)

### **Documentation**
- [ ] `docs/LLM_SETUP_GUIDE.md` (configuration)
- [ ] `docs/LLM_USAGE_GUIDE.md` (how agents use it)
- [ ] `docs/LLM_COST_MONITORING.md` (tracking costs)
- [ ] `docs/LLM_TROUBLESHOOTING.md` (common issues)
- [ ] Update `README.md` (add LLM capability)

### **Testing**
- [ ] Unit tests passing (15+ tests)
- [ ] Integration tests passing (5+ tests)
- [ ] Real-world tests (4 test projects)
- [ ] Performance benchmarks

---

## ğŸš€ **Next Steps**

Based on your execution order: **B â†’ D â†’ C â†’ A â†’ E**

### **âœ… Completed**
- âœ… **B) Detailed Implementation Plan** (this document)

### **ğŸ”œ Next: D) Proof-of-Concept Demo**

I'll create the POC demo that shows:
1. Gemini API connection
2. Code generation for a simple task
3. Quality comparison (template vs LLM)
4. Cost measurement
5. Validation that this approach works

**Ready to proceed with the POC demo?** ğŸ¯

---

**Plan Version**: 1.0  
**Date**: November 8, 2025  
**Estimated Total Effort**: 2-3 weeks (3 phases)  
**Phase 1 Effort**: 7-9 days  
**Status**: âœ… Ready for Execution

