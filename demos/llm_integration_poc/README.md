# LLM Integration - Proof of Concept Demo

**Date**: November 8, 2025  
**Purpose**: Validate LLM code generation works before full Q2O integration  
**Duration**: ~5 minutes to run

---

## ğŸ¯ **What This Demo Does**

This proof-of-concept demonstrates:

1. âœ… **Gemini API Connection** - Connects to Google's Gemini 1.5 Pro
2. âœ… **Code Generation** - Generates production-quality FastAPI code
3. âœ… **Quality Comparison** - Template vs LLM output
4. âœ… **Cost Measurement** - Actual token usage and costs
5. âœ… **Validation** - Checks for syntax, security, best practices

**Task**: Create a FastAPI Stripe webhook endpoint with signature verification

---

## ğŸš€ **Quick Start**

### **Step 1: Install Dependencies**

```bash
pip install google-generativeai python-dotenv
```

**Optional** (for OpenAI comparison):
```bash
pip install openai
```

### **Step 2: Configure API Key**

```bash
# Copy config template
cp config_example.txt .env

# Edit .env and add your Gemini API key
# Get key from: https://makersuite.google.com/app/apikey
```

Your `.env` should look like:
```
GOOGLE_API_KEY=AIzaSy...your_actual_key_here
USE_GEMINI=true
USE_OPENAI=false
```

### **Step 3: Run Demo**

```bash
cd demos/llm_integration_poc
python demo_llm_codegen.py
```

---

## ğŸ“Š **Expected Output**

```
================================================================================
                   Q2O LLM Integration - Proof of Concept
================================================================================

ğŸ“‹ Task: Create a FastAPI Stripe webhook endpoint with signature verification

================================================================================

[1/5] Template-Based Generation...
--------------------------------------------------------------------------------
âœ… Generated 487 characters
ğŸ“ Saved to: output/template_output.py

[2/5] LLM-Based Generation (Gemini 1.5 Pro)...
--------------------------------------------------------------------------------
âœ… Generated 1,245 characters
ğŸ“ Saved to: output/llm_gemini_output.py
ğŸ“Š Tokens: 856 input + 612 output
ğŸ’° Cost: $0.004134
â±ï¸  Duration: 2.34 seconds

[3/5] OpenAI Comparison (GPT-4 Turbo)...
--------------------------------------------------------------------------------
â­ï¸  Skipped (OpenAI disabled or not configured)

[4/5] Quality Comparison...
--------------------------------------------------------------------------------

ğŸ“Š Template Code:
   Lines: 29
   Quality Score: 71/100
   âœ… has_imports
   âœ… has_docstrings
   âŒ has_type_hints
   âŒ has_error_handling
   âŒ has_logging
   âœ… valid_syntax
   âœ… security_safe

ğŸ“Š LLM Code (Gemini):
   Lines: 47
   Quality Score: 100/100
   âœ… has_imports
   âœ… has_docstrings
   âœ… has_type_hints
   âœ… has_error_handling
   âœ… has_logging
   âœ… valid_syntax
   âœ… security_safe

ğŸ† Winner: LLM (LLM generated better code)

[5/5] Summary & Recommendations...
--------------------------------------------------------------------------------

âœ… PROOF OF CONCEPT SUCCESSFUL!

Key Findings:
  1. Gemini API connection: âœ… Working
  2. Code generation: âœ… Functional
  3. Cost per generation: $0.004134
  4. Response time: 2.34 seconds
  5. Quality: 100/100

ğŸ’° Cost Projections:
   Per file: $0.0041
   Per project (20 files): $0.08
   Per 10 projects: $0.82
   Per 100 projects: $8.23

ğŸ“‹ Recommendation:
   âœ… PROCEED with Phase 1 implementation
   âœ… Costs are within acceptable range
   âœ… Quality meets/exceeds template standards
   âœ… Using Gemini 1.5 Pro (87% cheaper than GPT-4)

================================================================================

ğŸ“„ Full results saved to: output/demo_results.json

Demo complete! Check the output/ directory for generated files.
```

---

## ğŸ“ **Output Files**

After running, check the `output/` directory:

| File | Description |
|------|-------------|
| `template_output.py` | Code generated from template (baseline) |
| `llm_gemini_output.py` | Code generated by Gemini 1.5 Pro |
| `llm_openai_output.py` | Code generated by GPT-4 (if enabled) |
| `demo_results.json` | Full results with usage stats |

---

## ğŸ§ª **Testing Different Scenarios**

### **Scenario 1: Simple Task** (Baseline)
```bash
# Default demo - Stripe webhook
python demo_llm_codegen.py
```
**Expected**: Low cost ($0.003-0.005), high quality

### **Scenario 2: Complex Task** (Edit demo script)
```python
task = """Create a complete GraphQL API with:
- User authentication (JWT)
- CRUD operations for Products
- Real-time subscriptions
- Rate limiting
- Comprehensive error handling
"""
```
**Expected**: Higher cost ($0.015-0.025), still high quality

### **Scenario 3: Cost Comparison**
Enable OpenAI in `.env`:
```
USE_OPENAI=true
```
**Expected**: OpenAI ~5-8x more expensive than Gemini

---

## ğŸ“‹ **Success Criteria**

The POC is successful if:

- âœ… Gemini API connects and generates code
- âœ… Generated code has valid Python syntax
- âœ… Cost is < $0.01 per generation
- âœ… Response time is < 5 seconds
- âœ… Quality score is 80%+ (7/7 checks pass)
- âœ… LLM code meets or exceeds template quality

---

## âš ï¸ **Troubleshooting**

### **Issue: "google-generativeai not installed"**
```bash
pip install google-generativeai
```

### **Issue: "GOOGLE_API_KEY not set"**
1. Get API key: https://makersuite.google.com/app/apikey
2. Add to `.env`: `GOOGLE_API_KEY=AIzaSy...`

### **Issue: "API key invalid"**
- Check key starts with "AIzaSy"
- No extra spaces or quotes
- Key is active in Google AI Studio

### **Issue: "Rate limit exceeded"**
- Free tier: 15 requests/minute
- Wait 1 minute and try again
- Or upgrade to paid tier

---

## ğŸ“Š **What to Look For**

### **In Generated Code**
- âœ… Complete type hints (`def func(x: str) -> int:`)
- âœ… Docstrings (Google style)
- âœ… Error handling (`try/except`)
- âœ… Security (webhook signature verification)
- âœ… Logging (structured logs)
- âœ… Pydantic validation
- âœ… Best practices (no hardcoded secrets, etc.)

### **In Cost Analysis**
- Input tokens: ~800-1,000 (prompt)
- Output tokens: ~500-700 (generated code)
- Total cost: $0.003-0.006 (Gemini)
- Duration: 2-4 seconds

---

## âœ… **Decision Point**

After running this demo, you'll know:

**If POC succeeds** (code quality good, cost acceptable):
â†’ âœ… **Proceed with Phase 1** (full CoderAgent integration)

**If POC fails** (quality issues, cost too high):
â†’ ğŸ”„ **Adjust approach** (try different models, prompts, or stick with templates)

---

## ğŸ¯ **Next Steps After POC**

1. **Review generated code** in `output/` directory
2. **Check cost projections** in terminal output
3. **Validate quality scores** (should be 90%+)
4. **Compare template vs LLM** output side-by-side
5. **Make decision**: Proceed or adjust?

If proceeding:
- Move to **Step C**: Set up production API keys
- Then **Step A**: Begin Phase 1 implementation

---

**POC Version**: 1.0  
**Status**: âœ… Ready to Run  
**Time Required**: ~5 minutes  
**Next**: Run demo, review results, make decision

