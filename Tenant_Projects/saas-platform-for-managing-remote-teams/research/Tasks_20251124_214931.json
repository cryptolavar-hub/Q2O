{
  "query": "* Tasks",
  "timestamp": "2025-11-24T21:49:31.224924",
  "depth": "deep",
  "search_results": [],
  "key_findings": [
    "\"https://docs.celeryq.dev/en/stable/\",",
    "\"https://www.rabbitmq.com/documentation.html\",",
    "\"https://docs.python.org/3/library/asyncio.html\",",
    "\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\",",
    "\"https://cloud.google.com/tasks/docs\"",
    "\"description\": \"Celery Task Definition and Invocation (Python)\",",
    "\"code\": \"from celery import Celery\\nimport time\\nimport requests\\n\\n# Assuming Redis as broker and backend\\napp = Celery('sports_data_tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')\\n\\n@app.task(bind=True, max_retries=5, default_retry_delay=30) # Retry up to 5 times, 30s delay\\ndef fetch_and_process_live_scores(self, api_endpoint: str, team_id: str):\\n    \\\"\\\"\\\"Fetches live scores for a team and processes them.\\\"\\\"\\\"\\n    try:\\n        response = requests.get(api_endpoint, params={'team_id': team_id}, timeout=10)\\n        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\\n        data = response.json()\\n        \\n        # Simulate processing (e.g., store in DB, calculate stats)\\n        print(f\\\"Processing live scores for team {team_id}: {data.get('scores')}\\\")\\n        time.sleep(2) # Simulate work\\n        \\n        # Example: Store result in a database (not shown here)\\n        # db.save_scores(team_id, data)\\n        \\n        return {'status': 'success', 'team_id': team_id, 'scores': data.get('scores')}\\n    except requests.exceptions.RequestException as e:\\n        print(f\\\"API call failed for {team_id}: {e}\\\")\\n        try:\\n            # Retry the task on network or API errors\\n            self.retry(exc=e)\\n        except self.MaxRetriesExceededError:\\n            print(f\\\"Max retries exceeded for task {self.request.id} for team {team_id}\\\")\\n            return {'status': 'failed', 'team_id': team_id, 'error': str(e)}\\n    except Exception as e:\\n        print(f\\\"An unexpected error occurred: {e}\\\")\\n        return {'status': 'failed', 'team_id': team_id, 'error': str(e)}\\n\\n# Example of how to call the task from a Flask/FastAPI endpoint or another script\\ndef trigger_score_update(team_id: str):\\n    api_url = \\\"https://api.sportsdata.example.com/v1/livescores\\\"\\n    task = fetch_and_process_live_scores.delay(api_url, team_id)\\n    print(f\\\"Task {task.id} triggered for team {team_id}\\\")\\n    return task.id\\n\\n# To run this example:\\n# 1. Install Celery, Redis, requests: pip install celery redis requests\\n# 2. Start Redis server\\n# 3. Start Celery worker: celery -A your_module_name worker --loglevel=info\\n# 4. Call trigger_score_update('LAL') from a Python interpreter or another script.\\n#    You can check task status: task_result = app.AsyncResult(task_id); print(task_result.state, task_result.result)\"",
    "\"description\": \"API Endpoint for Initiating a Background Task (FastAPI)\",",
    "\"code\": \"from fastapi import FastAPI, BackgroundTasks, HTTPException\\nfrom pydantic import BaseModel\\nfrom typing import Dict\\n\\n# Assuming 'app' is your Celery app from the previous example\\nfrom your_module_name import fetch_and_process_live_scores, app # Replace your_module_name\\n\\napi_app = FastAPI()\\n\\nclass TaskRequest(BaseModel):\\n    team_id: str\\n\\nclass TaskStatus(BaseModel):\\n    task_id: str\\n    status: str\\n    result: Dict = None\\n\\n@api_app.post(\\\"/tasks/fetch-scores\\\", status_code=202, response_model=Dict)\\nasync def initiate_score_fetch(request: TaskRequest):\\n    \\\"\\\"\\\"Initiates a background task to fetch and process live scores.\\\"\\\"\\\"\\n    task = fetch_and_process_live_scores.delay(\\\"https://api.sportsdata.example.com/v1/livescores\\\", request.team_id)\\n    return {\\\"message\\\": \\\"Score fetch initiated\\\", \\\"task_id\\\": task.id, \\\"status_url\\\": f\\\"/tasks/{task.id}/status\\\"}\\n\\n@api_app.get(\\\"/tasks/{task_id}/status\\\", response_model=TaskStatus)\\nasync def get_task_status(task_id: str):\\n    \\\"\\\"\\\"Retrieves the status of a background task.\\\"\\\"\\\"\\n    task_result = app.AsyncResult(task_id)\\n    if not task_result:\\n        raise HTTPException(status_code=404, detail=\\\"Task not found\\\")\\n    \\n    status = task_result.state\\n    result = None\\n    if task_result.ready(): # Task is finished (success or failure)\\n        try:\\n            result = task_result.get(timeout=1) # Get result, raises exception if task failed\\n        except Exception as e:\\n            result = {'error': str(e), 'traceback': task_result.traceback}\\n            status = 'FAILED'\\n    \\n    return {\\\"task_id\\\": task_id, \\\"status\\\": status, \\\"result\\\": result}\\n\\n# To run this example:\\n# 1. Ensure Celery setup from previous example is running.\\n# 2. Install FastAPI, Uvicorn: pip install fastapi uvicorn\\n# 3. Run Uvicorn: uvicorn your_module_name:api_app --reload\\n# 4. Access via browser or tool like Postman: POST /tasks/fetch-scores with body {\\\"team_id\\\": \\\"LAL\\\"}, then GET /tasks/{task_id}/status\"",
    "\"**Idempotent Tasks**: Design tasks to be idempotent, meaning executing them multiple times with the same inputs produces the same result without unintended side effects. This is crucial for retries and handling duplicate events.\","
  ],
  "documentation_urls": [
    "https://redis.io/docs/\",",
    "https://cloud.google.com/tasks/docs\"",
    "https://api.sportsdata.example.com/v1/livescores\\\"\\n",
    "https://api.sportsdata.example.com/v1/livescores\\\",",
    "https://docs.python.org/3/library/asyncio.html\",",
    "https://docs.celeryq.dev/en/stable/\",",
    "https://www.rabbitmq.com/documentation.html\",",
    "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\","
  ],
  "code_examples": [],
  "best_practices": [],
  "confidence_score": 45,
  "sources_consulted": [
    "llm_research_text",
    "llm_research"
  ],
  "cached": false
}